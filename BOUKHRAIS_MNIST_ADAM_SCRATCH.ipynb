{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oa63KejBfog8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvrLumwPhZrN"
   },
   "source": [
    "## 1. Chargement des données et Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oPZCV7XTfucK",
    "outputId": "aea04f64-37bd-4a81-bd9a-d36b79e4f13b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset: (60000, 28, 28)\n",
      "testset: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Chargement des données MNIST\n",
    "(X_train, y_train) , (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print('trainset:', X_train.shape) # 60,000 images\n",
    "print('testset:', X_test.shape) # 10,000 images\n",
    "\n",
    "# Normalisation des données\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKEjEqbOhdh0"
   },
   "source": [
    "## 2. Visualisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "BmTNBz7bgeZG",
    "outputId": "95ff4814-9e82-488a-c1b2-a8febd82f019"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualisation de quelques images\n",
    "fig, ax = plt.subplots(nrows=1, ncols=10, figsize=(20, 4))\n",
    "for i in range(10):\n",
    "  ax[i].imshow(X_train[i], cmap='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FInfYGsMhflz"
   },
   "source": [
    "## 3. Configuration des Couches du Réseau de Neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RFfzviJKg5WE"
   },
   "outputs": [],
   "source": [
    "# Configuration des couches du réseau\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfdDh4_zhkL7"
   },
   "source": [
    "## 4. Entrainement du Réseau de Neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class AdamOptim():\n",
    "    def __init__(self, eta=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.m_dw, self.v_dw = 0, 0\n",
    "        self.m_db, self.v_db = 0, 0\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.eta = eta\n",
    "    def update(self, t, w, b, dw, db):\n",
    "        ## dw, db are from current minibatch\n",
    "        ## momentum beta 1\n",
    "        # *** weights *** #\n",
    "        self.m_dw = self.beta1*self.m_dw + (1-self.beta1)*dw\n",
    "        # *** biases *** #\n",
    "        self.m_db = self.beta1*self.m_db + (1-self.beta1)*db\n",
    "\n",
    "        ## rms beta 2\n",
    "        # *** weights *** #\n",
    "        self.v_dw = self.beta2*self.v_dw + (1-self.beta2)*(dw**2)\n",
    "        # *** biases *** #\n",
    "        self.v_db = self.beta2*self.v_db + (1-self.beta2)*(db)\n",
    "\n",
    "        ## bias correction\n",
    "        m_dw_corr = self.m_dw/(1-self.beta1**t)\n",
    "        m_db_corr = self.m_db/(1-self.beta1**t)\n",
    "        v_dw_corr = self.v_dw/(1-self.beta2**t)\n",
    "        v_db_corr = self.v_db/(1-self.beta2**t)\n",
    "\n",
    "        ## update weights and biases\n",
    "        w = w - self.eta*(m_dw_corr/(np.sqrt(v_dw_corr)+self.epsilon))\n",
    "        b = b - self.eta*(m_db_corr/(np.sqrt(v_db_corr)+self.epsilon))\n",
    "        return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(m):\n",
    "    return m**2-2*m+1\n",
    "## take derivative\n",
    "def grad_function(m):\n",
    "    return 2*m-2\n",
    "def check_convergence(w0, w1):\n",
    "    return (w0 == w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1: weight=0.009999999950000001\n",
      "iteration 2: weight=0.01999725400385255\n",
      "iteration 3: weight=0.029989900621600046\n",
      "iteration 4: weight=0.039976060276935343\n",
      "iteration 5: weight=0.049953839711732076\n",
      "iteration 6: weight=0.05992133621693422\n",
      "iteration 7: weight=0.06987664190678831\n",
      "iteration 8: weight=0.07981784795404925\n",
      "iteration 9: weight=0.08974304875491491\n",
      "iteration 10: weight=0.0996503459940126\n",
      "iteration 11: weight=0.10953785258172263\n",
      "iteration 12: weight=0.11940369643843479\n",
      "iteration 13: weight=0.12924602410293135\n",
      "iteration 14: weight=0.13906300414491304\n",
      "iteration 15: weight=0.14885283036466956\n",
      "iteration 16: weight=0.15861372476597732\n",
      "iteration 17: weight=0.1683439402914239\n",
      "iteration 18: weight=0.17804176331244895\n",
      "iteration 19: weight=0.1877055158694015\n",
      "iteration 20: weight=0.19733355765979776\n",
      "iteration 21: weight=0.2069242877756729\n",
      "iteration 22: weight=0.21647614619342795\n",
      "iteration 23: weight=0.22598761502184558\n",
      "iteration 24: weight=0.23545721951596985\n",
      "iteration 25: weight=0.24488352886630008\n",
      "iteration 26: weight=0.25426515677423506\n",
      "iteration 27: weight=0.26360076182591813\n",
      "iteration 28: weight=0.2728890476775851\n",
      "iteration 29: weight=0.2821287630662142\n",
      "iteration 30: weight=0.2913187016597368\n",
      "iteration 31: weight=0.3004577017613055\n",
      "iteration 32: weight=0.30954464588215314\n",
      "iteration 33: weight=0.3185784601974346\n",
      "iteration 34: weight=0.32755811389914286\n",
      "iteration 35: weight=0.3364826184597571\n",
      "iteration 36: weight=0.3453510268197323\n",
      "iteration 37: weight=0.3541624325113025\n",
      "iteration 38: weight=0.36291596873035775\n",
      "iteration 39: weight=0.3716108073673929\n",
      "iteration 40: weight=0.38024615800772815\n",
      "iteration 41: weight=0.3888212669103811\n",
      "iteration 42: weight=0.3973354159741451\n",
      "iteration 43: weight=0.405787921698609\n",
      "iteration 44: weight=0.4141781341470465\n",
      "iteration 45: weight=0.42250543591732403\n",
      "iteration 46: weight=0.4307692411262221\n",
      "iteration 47: weight=0.4389689944118521\n",
      "iteration 48: weight=0.44710416995817326\n",
      "iteration 49: weight=0.455174270544982\n",
      "iteration 50: weight=0.4631788266261575\n",
      "iteration 51: weight=0.47111739543840325\n",
      "iteration 52: weight=0.47898956014222716\n",
      "iteration 53: weight=0.4867949289964492\n",
      "iteration 54: weight=0.4945331345671171\n",
      "iteration 55: weight=0.502203832971342\n",
      "iteration 56: weight=0.5098067031562412\n",
      "iteration 57: weight=0.5173414462128868\n",
      "iteration 58: weight=0.5248077847249037\n",
      "iteration 59: weight=0.5322054621511477\n",
      "iteration 60: weight=0.5395342422417011\n",
      "iteration 61: weight=0.5467939084862693\n",
      "iteration 62: weight=0.5539842635939275\n",
      "iteration 63: weight=0.5611051290030595\n",
      "iteration 64: weight=0.568156344420244\n",
      "iteration 65: weight=0.5751377673867769\n",
      "iteration 66: weight=0.5820492728714711\n",
      "iteration 67: weight=0.5888907528883383\n",
      "iteration 68: weight=0.5956621161377404\n",
      "iteration 69: weight=0.6023632876695909\n",
      "iteration 70: weight=0.6089942085671857\n",
      "iteration 71: weight=0.615554835650261\n",
      "iteration 72: weight=0.6220451411958915\n",
      "iteration 73: weight=0.6284651126758695\n",
      "iteration 74: weight=0.6348147525092407\n",
      "iteration 75: weight=0.641094077828706\n",
      "iteration 76: weight=0.6473031202596423\n",
      "iteration 77: weight=0.653441925710538\n",
      "iteration 78: weight=0.6595105541736849\n",
      "iteration 79: weight=0.6655090795350186\n",
      "iteration 80: weight=0.6714375893920466\n",
      "iteration 81: weight=0.6772961848788547\n",
      "iteration 82: weight=0.6830849804972335\n",
      "iteration 83: weight=0.6888041039530174\n",
      "iteration 84: weight=0.6944536959967778\n",
      "iteration 85: weight=0.7000339102680659\n",
      "iteration 86: weight=0.7055449131424467\n",
      "iteration 87: weight=0.7109868835806149\n",
      "iteration 88: weight=0.7163600129789347\n",
      "iteration 89: weight=0.7216645050207873\n",
      "iteration 90: weight=0.7269005755281592\n",
      "iteration 91: weight=0.7320684523129467\n",
      "iteration 92: weight=0.7371683750274948\n",
      "iteration 93: weight=0.7422005950139324\n",
      "iteration 94: weight=0.7471653751519031\n",
      "iteration 95: weight=0.7520629897043302\n",
      "iteration 96: weight=0.7568937241608946\n",
      "iteration 97: weight=0.7616578750789351\n",
      "iteration 98: weight=0.7663557499215196\n",
      "iteration 99: weight=0.7709876668924653\n",
      "iteration 100: weight=0.7755539547681211\n",
      "iteration 101: weight=0.7800549527257501\n",
      "iteration 102: weight=0.7844910101683854\n",
      "iteration 103: weight=0.7888624865460541\n",
      "iteration 104: weight=0.7931697511732935\n",
      "iteration 105: weight=0.7974131830429079\n",
      "iteration 106: weight=0.8015931706359366\n",
      "iteration 107: weight=0.8057101117278274\n",
      "iteration 108: weight=0.8097644131908287\n",
      "iteration 109: weight=0.8137564907926365\n",
      "iteration 110: weight=0.8176867689913462\n",
      "iteration 111: weight=0.8215556807267823\n",
      "iteration 112: weight=0.8253636672082894\n",
      "iteration 113: weight=0.8291111776990878\n",
      "iteration 114: weight=0.8327986692973092\n",
      "iteration 115: weight=0.8364266067138391\n",
      "iteration 116: weight=0.8399954620471094\n",
      "iteration 117: weight=0.8435057145549907\n",
      "iteration 118: weight=0.846957850423946\n",
      "iteration 119: weight=0.8503523625356184\n",
      "iteration 120: weight=0.8536897502310298\n",
      "iteration 121: weight=0.8569705190725782\n",
      "iteration 122: weight=0.860195180604026\n",
      "iteration 123: weight=0.8633642521086788\n",
      "iteration 124: weight=0.8664782563659569\n",
      "iteration 125: weight=0.8695377214065675\n",
      "iteration 126: weight=0.8725431802664898\n",
      "iteration 127: weight=0.8754951707399848\n",
      "iteration 128: weight=0.8783942351318468\n",
      "iteration 129: weight=0.8812409200091125\n",
      "iteration 130: weight=0.8840357759524461\n",
      "iteration 131: weight=0.8867793573074179\n",
      "iteration 132: weight=0.8894722219358927\n",
      "iteration 133: weight=0.8921149309677457\n",
      "iteration 134: weight=0.8947080485531188\n",
      "iteration 135: weight=0.8972521416154303\n",
      "iteration 136: weight=0.8997477796053487\n",
      "iteration 137: weight=0.9021955342559357\n",
      "iteration 138: weight=0.9045959793391636\n",
      "iteration 139: weight=0.9069496904240053\n",
      "iteration 140: weight=0.9092572446362935\n",
      "iteration 141: weight=0.9115192204205393\n",
      "iteration 142: weight=0.9137361973038962\n",
      "iteration 143: weight=0.9159087556624508\n",
      "iteration 144: weight=0.918037476490016\n",
      "iteration 145: weight=0.9201229411695963\n",
      "iteration 146: weight=0.9221657312476885\n",
      "iteration 147: weight=0.9241664282115778\n",
      "iteration 148: weight=0.9261256132697787\n",
      "iteration 149: weight=0.9280438671357686\n",
      "iteration 150: weight=0.9299217698151505\n",
      "iteration 151: weight=0.9317599003963796\n",
      "iteration 152: weight=0.933558836845177\n",
      "iteration 153: weight=0.9353191558027519\n",
      "iteration 154: weight=0.9370414323879417\n",
      "iteration 155: weight=0.938726240003377\n",
      "iteration 156: weight=0.9403741501457696\n",
      "iteration 157: weight=0.9419857322204138\n",
      "iteration 158: weight=0.9435615533599866\n",
      "iteration 159: weight=0.9451021782477242\n",
      "iteration 160: weight=0.946608168945046\n",
      "iteration 161: weight=0.9480800847236891\n",
      "iteration 162: weight=0.9495184819024132\n",
      "iteration 163: weight=0.950923913688324\n",
      "iteration 164: weight=0.9522969300228624\n",
      "iteration 165: weight=0.9536380774324956\n",
      "iteration 166: weight=0.9549478988841421\n",
      "iteration 167: weight=0.9562269336453583\n",
      "iteration 168: weight=0.9574757171493036\n",
      "iteration 169: weight=0.9586947808645\n",
      "iteration 170: weight=0.9598846521693933\n",
      "iteration 171: weight=0.9610458542317184\n",
      "iteration 172: weight=0.9621789058926664\n",
      "iteration 173: weight=0.9632843215558446\n",
      "iteration 174: weight=0.9643626110810171\n",
      "iteration 175: weight=0.9654142796826064\n",
      "iteration 176: weight=0.9664398278329352\n",
      "iteration 177: weight=0.9674397511701782\n",
      "iteration 178: weight=0.9684145404109944\n",
      "iteration 179: weight=0.9693646812678026\n",
      "iteration 180: weight=0.9702906543706602\n",
      "iteration 181: weight=0.9711929351937009\n",
      "iteration 182: weight=0.9720719939860842\n",
      "iteration 183: weight=0.9729282957074055\n",
      "iteration 184: weight=0.9737622999675106\n",
      "iteration 185: weight=0.9745744609706605\n",
      "iteration 186: weight=0.9753652274639815\n",
      "iteration 187: weight=0.9761350426901402\n",
      "iteration 188: weight=0.9768843443441767\n",
      "iteration 189: weight=0.9776135645344266\n",
      "iteration 190: weight=0.9783231297474617\n",
      "iteration 191: weight=0.9790134608169765\n",
      "iteration 192: weight=0.9796849728965454\n",
      "iteration 193: weight=0.9803380754361745\n",
      "iteration 194: weight=0.9809731721625682\n",
      "iteration 195: weight=0.9815906610630333\n",
      "iteration 196: weight=0.9821909343729368\n",
      "iteration 197: weight=0.9827743785666372\n",
      "iteration 198: weight=0.9833413743518047\n",
      "iteration 199: weight=0.9838922966670466\n",
      "iteration 200: weight=0.984427514682753\n",
      "iteration 201: weight=0.9849473918050764\n",
      "iteration 202: weight=0.98545228568296\n",
      "iteration 203: weight=0.985942548218127\n",
      "iteration 204: weight=0.9864185255779455\n",
      "iteration 205: weight=0.9868805582110803\n",
      "iteration 206: weight=0.9873289808658458\n",
      "iteration 207: weight=0.9877641226111719\n",
      "iteration 208: weight=0.9881863068600975\n",
      "iteration 209: weight=0.988595851395703\n",
      "iteration 210: weight=0.9889930683993977\n",
      "iteration 211: weight=0.9893782644814731\n",
      "iteration 212: weight=0.9897517407138409\n",
      "iteration 213: weight=0.9901137926648667\n",
      "iteration 214: weight=0.9904647104362185\n",
      "iteration 215: weight=0.9908047787016447\n",
      "iteration 216: weight=0.9911342767475994\n",
      "iteration 217: weight=0.9914534785156336\n",
      "iteration 218: weight=0.9917626526464719\n",
      "iteration 219: weight=0.9920620625256936\n",
      "iteration 220: weight=0.9923519663309407\n",
      "iteration 221: weight=0.9926326170805743\n",
      "iteration 222: weight=0.9929042626837034\n",
      "iteration 223: weight=0.9931671459915103\n",
      "iteration 224: weight=0.9934215048497993\n",
      "iteration 225: weight=0.993667572152694\n",
      "iteration 226: weight=0.993905575897414\n",
      "iteration 227: weight=0.9941357392400594\n",
      "iteration 228: weight=0.9943582805523336\n",
      "iteration 229: weight=0.9945734134791384\n",
      "iteration 230: weight=0.9947813469969744\n",
      "iteration 231: weight=0.9949822854730818\n",
      "iteration 232: weight=0.9951764287252586\n",
      "iteration 233: weight=0.995363972082295\n",
      "iteration 234: weight=0.9955451064449625\n",
      "iteration 235: weight=0.9957200183474998\n",
      "iteration 236: weight=0.9958888900195372\n",
      "iteration 237: weight=0.9960518994484044\n",
      "iteration 238: weight=0.9962092204417675\n",
      "iteration 239: weight=0.9963610226905409\n",
      "iteration 240: weight=0.9965074718320245\n",
      "iteration 241: weight=0.9966487295132145\n",
      "iteration 242: weight=0.9967849534542412\n",
      "iteration 243: weight=0.9969162975118857\n",
      "iteration 244: weight=0.99704291174313\n",
      "iteration 245: weight=0.9971649424686971\n",
      "iteration 246: weight=0.9972825323365392\n",
      "iteration 247: weight=0.9973958203852304\n",
      "iteration 248: weight=0.9975049421072285\n",
      "iteration 249: weight=0.9976100295119631\n",
      "iteration 250: weight=0.9977112111887169\n",
      "iteration 251: weight=0.997808612369263\n",
      "iteration 252: weight=0.9979023549902253\n",
      "iteration 253: weight=0.997992557755128\n",
      "iteration 254: weight=0.9980793361961048\n",
      "iteration 255: weight=0.9981628027352366\n",
      "iteration 256: weight=0.9982430667454899\n",
      "iteration 257: weight=0.9983202346112277\n",
      "iteration 258: weight=0.9983944097882684\n",
      "iteration 259: weight=0.9984656928634669\n",
      "iteration 260: weight=0.998534181613794\n",
      "iteration 261: weight=0.9985999710648938\n",
      "iteration 262: weight=0.9986631535490955\n",
      "iteration 263: weight=0.9987238187628614\n",
      "iteration 264: weight=0.9987820538236523\n",
      "iteration 265: weight=0.9988379433261909\n",
      "iteration 266: weight=0.9988915693981099\n",
      "iteration 267: weight=0.9989430117549652\n",
      "iteration 268: weight=0.9989923477546039\n",
      "iteration 269: weight=0.9990396524508698\n",
      "iteration 270: weight=0.9990849986466379\n",
      "iteration 271: weight=0.9991284569461627\n",
      "iteration 272: weight=0.9991700958067327\n",
      "iteration 273: weight=0.9992099815896199\n",
      "iteration 274: weight=0.9992481786103167\n",
      "iteration 275: weight=0.9992847491880505\n",
      "iteration 276: weight=0.9993197536945726\n",
      "iteration 277: weight=0.9993532506022106\n",
      "iteration 278: weight=0.9993852965311832\n",
      "iteration 279: weight=0.999415946296171\n",
      "iteration 280: weight=0.9994452529521384\n",
      "iteration 281: weight=0.9994732678394059\n",
      "iteration 282: weight=0.999500040627969\n",
      "iteration 283: weight=0.9995256193610621\n",
      "iteration 284: weight=0.9995500504979664\n",
      "iteration 285: weight=0.9995733789560621\n",
      "iteration 286: weight=0.9995956481521241\n",
      "iteration 287: weight=0.9996169000428623\n",
      "iteration 288: weight=0.9996371751647086\n",
      "iteration 289: weight=0.9996565126728514\n",
      "iteration 290: weight=0.9996749503795203\n",
      "iteration 291: weight=0.9996925247915246\n",
      "iteration 292: weight=0.9997092711470486\n",
      "iteration 293: weight=0.9997252234517061\n",
      "iteration 294: weight=0.9997404145138615\n",
      "iteration 295: weight=0.9997548759792192\n",
      "iteration 296: weight=0.9997686383646873\n",
      "iteration 297: weight=0.9997817310915225\n",
      "iteration 298: weight=0.9997941825177595\n",
      "iteration 299: weight=0.9998060199699335\n",
      "iteration 300: weight=0.9998172697740993\n",
      "iteration 301: weight=0.9998279572861581\n",
      "iteration 302: weight=0.9998381069214939\n",
      "iteration 303: weight=0.9998477421839309\n",
      "iteration 304: weight=0.9998568856940179\n",
      "iteration 305: weight=0.9998655592166464\n",
      "iteration 306: weight=0.9998737836880133\n",
      "iteration 307: weight=0.9998815792419323\n",
      "iteration 308: weight=0.9998889652355071\n",
      "iteration 309: weight=0.9998959602741706\n",
      "iteration 310: weight=0.9999025822361022\n",
      "iteration 311: weight=0.9999088482960293\n",
      "iteration 312: weight=0.9999147749484246\n",
      "iteration 313: weight=0.999920378030106\n",
      "iteration 314: weight=0.9999256727422497\n",
      "iteration 315: weight=0.9999306736718255\n",
      "iteration 316: weight=0.9999353948124632\n",
      "iteration 317: weight=0.9999398495847598\n",
      "iteration 318: weight=0.9999440508560381\n",
      "iteration 319: weight=0.9999480109595638\n",
      "iteration 320: weight=0.9999517417132329\n",
      "iteration 321: weight=0.9999552544377384\n",
      "iteration 322: weight=0.9999585599742249\n",
      "iteration 323: weight=0.9999616687014424\n",
      "iteration 324: weight=0.9999645905524075\n",
      "iteration 325: weight=0.999967335030582\n",
      "iteration 326: weight=0.9999699112255794\n",
      "iteration 327: weight=0.9999723278284075\n",
      "iteration 328: weight=0.9999745931462574\n",
      "iteration 329: weight=0.9999767151168482\n",
      "iteration 330: weight=0.9999787013223368\n",
      "iteration 331: weight=0.9999805590028027\n",
      "iteration 332: weight=0.9999822950693158\n",
      "iteration 333: weight=0.9999839161165983\n",
      "iteration 334: weight=0.9999854284352881\n",
      "iteration 335: weight=0.9999868380238144\n",
      "iteration 336: weight=0.9999881505998927\n",
      "iteration 337: weight=0.9999893716116505\n",
      "iteration 338: weight=0.9999905062483903\n",
      "iteration 339: weight=0.9999915594509997\n",
      "iteration 340: weight=0.9999925359220176\n",
      "iteration 341: weight=0.9999934401353642\n",
      "iteration 342: weight=0.9999942763457434\n",
      "iteration 343: weight=0.9999950485977274\n",
      "iteration 344: weight=0.9999957607345287\n",
      "iteration 345: weight=0.999996416406471\n",
      "iteration 346: weight=0.9999970190791647\n",
      "iteration 347: weight=0.9999975720413955\n",
      "iteration 348: weight=0.9999980784127344\n",
      "iteration 349: weight=0.9999985411508757\n",
      "iteration 350: weight=0.9999989630587119\n",
      "iteration 351: weight=0.9999993467911512\n",
      "iteration 352: weight=0.9999996948616862\n",
      "iteration 353: weight=1.0000000096487203\n",
      "iteration 354: weight=1.0000002934016596\n",
      "iteration 355: weight=1.0000005482467753\n",
      "iteration 356: weight=1.0000007761928456\n",
      "iteration 357: weight=1.0000009791365825\n",
      "iteration 358: weight=1.0000011588678495\n",
      "iteration 359: weight=1.0000013170746784\n",
      "iteration 360: weight=1.0000014553480892\n",
      "iteration 361: weight=1.0000015751867204\n",
      "iteration 362: weight=1.0000016780012766\n",
      "iteration 363: weight=1.0000017651187962\n",
      "iteration 364: weight=1.0000018377867486\n",
      "iteration 365: weight=1.0000018971769635\n",
      "iteration 366: weight=1.0000019443894\n",
      "iteration 367: weight=1.0000019804557585\n",
      "iteration 368: weight=1.0000020063429436\n",
      "iteration 369: weight=1.0000020229563793\n",
      "iteration 370: weight=1.0000020311431854\n",
      "iteration 371: weight=1.0000020316952167\n",
      "iteration 372: weight=1.0000020253519715\n",
      "iteration 373: weight=1.0000020128033733\n",
      "iteration 374: weight=1.0000019946924315\n",
      "iteration 375: weight=1.0000019716177821\n",
      "iteration 376: weight=1.0000019441361176\n",
      "iteration 377: weight=1.0000019127645048\n",
      "iteration 378: weight=1.000001877982599\n",
      "iteration 379: weight=1.0000018402347561\n",
      "iteration 380: weight=1.0000017999320474\n",
      "iteration 381: weight=1.000001757454179\n",
      "iteration 382: weight=1.0000017131513226\n",
      "iteration 383: weight=1.0000016673458578\n",
      "iteration 384: weight=1.000001620334032\n",
      "iteration 385: weight=1.0000015723875386\n",
      "iteration 386: weight=1.0000015237550193\n",
      "iteration 387: weight=1.0000014746634915\n",
      "iteration 388: weight=1.0000014253197047\n",
      "iteration 389: weight=1.0000013759114286\n",
      "iteration 390: weight=1.0000013266086762\n",
      "iteration 391: weight=1.0000012775648637\n",
      "iteration 392: weight=1.0000012289179103\n",
      "iteration 393: weight=1.0000011807912805\n",
      "iteration 394: weight=1.0000011332949712\n",
      "iteration 395: weight=1.000001086526446\n",
      "iteration 396: weight=1.000001040571519\n",
      "iteration 397: weight=1.0000009955051905\n",
      "iteration 398: weight=1.0000009513924366\n",
      "iteration 399: weight=1.0000009082889536\n",
      "iteration 400: weight=1.0000008662418622\n",
      "iteration 401: weight=1.0000008252903696\n",
      "iteration 402: weight=1.0000007854663946\n",
      "iteration 403: weight=1.0000007467951555\n",
      "iteration 404: weight=1.000000709295723\n",
      "iteration 405: weight=1.0000006729815407\n",
      "iteration 406: weight=1.0000006378609128\n",
      "iteration 407: weight=1.0000006039374625\n",
      "iteration 408: weight=1.0000005712105615\n",
      "iteration 409: weight=1.0000005396757328\n",
      "iteration 410: weight=1.0000005093250262\n",
      "iteration 411: weight=1.0000004801473712\n",
      "iteration 412: weight=1.0000004521289048\n",
      "iteration 413: weight=1.0000004252532784\n",
      "iteration 414: weight=1.0000003995019433\n",
      "iteration 415: weight=1.000000374854416\n",
      "iteration 416: weight=1.0000003512885252\n",
      "iteration 417: weight=1.000000328780641\n",
      "iteration 418: weight=1.0000003073058867\n",
      "iteration 419: weight=1.0000002868383355\n",
      "iteration 420: weight=1.0000002673511916\n",
      "iteration 421: weight=1.000000248816957\n",
      "iteration 422: weight=1.000000231207586\n",
      "iteration 423: weight=1.0000002144946256\n",
      "iteration 424: weight=1.0000001986493454\n",
      "iteration 425: weight=1.0000001836428556\n",
      "iteration 426: weight=1.0000001694462146\n",
      "iteration 427: weight=1.0000001560305274\n",
      "iteration 428: weight=1.0000001433670334\n",
      "iteration 429: weight=1.0000001314271876\n",
      "iteration 430: weight=1.0000001201827318\n",
      "iteration 431: weight=1.0000001096057591\n",
      "iteration 432: weight=1.000000099668772\n",
      "iteration 433: weight=1.000000090344732\n",
      "iteration 434: weight=1.000000081607105\n",
      "iteration 435: weight=1.0000000734299008\n",
      "iteration 436: weight=1.0000000657877053\n",
      "iteration 437: weight=1.0000000586557103\n",
      "iteration 438: weight=1.0000000520097374\n",
      "iteration 439: weight=1.0000000458262583\n",
      "iteration 440: weight=1.0000000400824103\n",
      "iteration 441: weight=1.000000034756009\n",
      "iteration 442: weight=1.0000000298255576\n",
      "iteration 443: weight=1.0000000252702532\n",
      "iteration 444: weight=1.0000000210699902\n",
      "iteration 445: weight=1.0000000172053607\n",
      "iteration 446: weight=1.0000000136576537\n",
      "iteration 447: weight=1.0000000104088511\n",
      "iteration 448: weight=1.000000007441623\n",
      "iteration 449: weight=1.0000000047393205\n",
      "iteration 450: weight=1.0000000022859659\n",
      "iteration 451: weight=1.0000000000662446\n",
      "iteration 452: weight=0.9999999980654928\n",
      "iteration 453: weight=0.9999999962696857\n",
      "iteration 454: weight=0.9999999946654243\n",
      "iteration 455: weight=0.9999999932399216\n",
      "iteration 456: weight=0.9999999919809877\n",
      "iteration 457: weight=0.9999999908770149\n",
      "iteration 458: weight=0.999999989916962\n",
      "iteration 459: weight=0.9999999890903385\n",
      "iteration 460: weight=0.9999999883871876\n",
      "iteration 461: weight=0.9999999877980703\n",
      "iteration 462: weight=0.9999999873140484\n",
      "iteration 463: weight=0.9999999869266677\n",
      "iteration 464: weight=0.9999999866279413\n",
      "iteration 465: weight=0.9999999864103329\n",
      "iteration 466: weight=0.9999999862667397\n",
      "iteration 467: weight=0.9999999861904763\n",
      "iteration 468: weight=0.9999999861752578\n",
      "iteration 469: weight=0.9999999862151839\n",
      "iteration 470: weight=0.9999999863047233\n",
      "iteration 471: weight=0.9999999864386969\n",
      "iteration 472: weight=0.9999999866122634\n",
      "iteration 473: weight=0.9999999868209039\n",
      "iteration 474: weight=0.9999999870604066\n",
      "iteration 475: weight=0.9999999873268534\n",
      "iteration 476: weight=0.9999999876166047\n",
      "iteration 477: weight=0.9999999879262865\n",
      "iteration 478: weight=0.9999999882527769\n",
      "iteration 479: weight=0.999999988593193\n",
      "iteration 480: weight=0.9999999889448784\n",
      "iteration 481: weight=0.9999999893053915\n",
      "iteration 482: weight=0.9999999896724932\n",
      "iteration 483: weight=0.9999999900441359\n",
      "iteration 484: weight=0.9999999904184526\n",
      "iteration 485: weight=0.9999999907937464\n",
      "iteration 486: weight=0.9999999911684802\n",
      "iteration 487: weight=0.9999999915412672\n",
      "iteration 488: weight=0.9999999919108616\n",
      "iteration 489: weight=0.9999999922761498\n",
      "iteration 490: weight=0.9999999926361411\n",
      "iteration 491: weight=0.9999999929899606\n",
      "iteration 492: weight=0.9999999933368405\n",
      "iteration 493: weight=0.9999999936761135\n",
      "iteration 494: weight=0.9999999940072047\n",
      "iteration 495: weight=0.9999999943296259\n",
      "iteration 496: weight=0.9999999946429682\n",
      "iteration 497: weight=0.9999999949468965\n",
      "iteration 498: weight=0.9999999952411437\n",
      "iteration 499: weight=0.999999995525505\n",
      "iteration 500: weight=0.9999999957998327\n",
      "iteration 501: weight=0.9999999960640314\n",
      "iteration 502: weight=0.9999999963180537\n",
      "iteration 503: weight=0.999999996561895\n",
      "iteration 504: weight=0.9999999967955902\n",
      "iteration 505: weight=0.9999999970192094\n",
      "iteration 506: weight=0.9999999972328546\n",
      "iteration 507: weight=0.9999999974366559\n",
      "iteration 508: weight=0.9999999976307687\n",
      "iteration 509: weight=0.9999999978153705\n",
      "iteration 510: weight=0.9999999979906583\n",
      "iteration 511: weight=0.9999999981568457\n",
      "iteration 512: weight=0.999999998314161\n",
      "iteration 513: weight=0.9999999984628445\n",
      "iteration 514: weight=0.9999999986031467\n",
      "iteration 515: weight=0.9999999987353264\n",
      "iteration 516: weight=0.9999999988596489\n",
      "iteration 517: weight=0.9999999989763841\n",
      "iteration 518: weight=0.9999999990858058\n",
      "iteration 519: weight=0.9999999991881894\n",
      "iteration 520: weight=0.9999999992838112\n",
      "iteration 521: weight=0.9999999993729474\n",
      "iteration 522: weight=0.9999999994558726\n",
      "iteration 523: weight=0.9999999995328595\n",
      "iteration 524: weight=0.9999999996041773\n",
      "iteration 525: weight=0.9999999996700918\n",
      "iteration 526: weight=0.9999999997308642\n",
      "iteration 527: weight=0.9999999997867507\n",
      "iteration 528: weight=0.999999999838002\n",
      "iteration 529: weight=0.9999999998848629\n",
      "iteration 530: weight=0.9999999999275718\n",
      "iteration 531: weight=0.9999999999663607\n",
      "iteration 532: weight=1.0000000000014544\n",
      "iteration 533: weight=1.0000000000330709\n",
      "iteration 534: weight=1.0000000000614209\n",
      "iteration 535: weight=1.0000000000867075\n",
      "iteration 536: weight=1.0000000001091267\n",
      "iteration 537: weight=1.000000000128867\n",
      "iteration 538: weight=1.0000000001461087\n",
      "iteration 539: weight=1.0000000001610256\n",
      "iteration 540: weight=1.0000000001737834\n",
      "iteration 541: weight=1.0000000001845408\n",
      "iteration 542: weight=1.0000000001934488\n",
      "iteration 543: weight=1.0000000002006517\n",
      "iteration 544: weight=1.0000000002062863\n",
      "iteration 545: weight=1.0000000002104827\n",
      "iteration 546: weight=1.0000000002133642\n",
      "iteration 547: weight=1.0000000002150473\n",
      "iteration 548: weight=1.0000000002156426\n",
      "iteration 549: weight=1.0000000002152538\n",
      "iteration 550: weight=1.0000000002139788\n",
      "iteration 551: weight=1.0000000002119098\n",
      "iteration 552: weight=1.0000000002091332\n",
      "iteration 553: weight=1.0000000002057299\n",
      "iteration 554: weight=1.0000000002017755\n",
      "iteration 555: weight=1.0000000001973406\n",
      "iteration 556: weight=1.000000000192491\n",
      "iteration 557: weight=1.0000000001872875\n",
      "iteration 558: weight=1.0000000001817868\n",
      "iteration 559: weight=1.0000000001760414\n",
      "iteration 560: weight=1.0000000001700993\n",
      "iteration 561: weight=1.000000000164005\n",
      "iteration 562: weight=1.0000000001577996\n",
      "iteration 563: weight=1.00000000015152\n",
      "iteration 564: weight=1.0000000001452\n",
      "iteration 565: weight=1.000000000138871\n",
      "iteration 566: weight=1.0000000001325604\n",
      "iteration 567: weight=1.0000000001262936\n",
      "iteration 568: weight=1.000000000120093\n",
      "iteration 569: weight=1.0000000001139786\n",
      "iteration 570: weight=1.000000000107968\n",
      "iteration 571: weight=1.0000000001020772\n",
      "iteration 572: weight=1.0000000000963196\n",
      "iteration 573: weight=1.0000000000907068\n",
      "iteration 574: weight=1.000000000085249\n",
      "iteration 575: weight=1.0000000000799543\n",
      "iteration 576: weight=1.0000000000748297\n",
      "iteration 577: weight=1.0000000000698805\n",
      "iteration 578: weight=1.0000000000651112\n",
      "iteration 579: weight=1.0000000000605247\n",
      "iteration 580: weight=1.0000000000561229\n",
      "iteration 581: weight=1.000000000051907\n",
      "iteration 582: weight=1.000000000047877\n",
      "iteration 583: weight=1.0000000000440323\n",
      "iteration 584: weight=1.0000000000403715\n",
      "iteration 585: weight=1.0000000000368925\n",
      "iteration 586: weight=1.0000000000335927\n",
      "iteration 587: weight=1.000000000030469\n",
      "iteration 588: weight=1.0000000000275175\n",
      "iteration 589: weight=1.0000000000247347\n",
      "iteration 590: weight=1.000000000022116\n",
      "iteration 591: weight=1.0000000000196572\n",
      "iteration 592: weight=1.000000000017353\n",
      "iteration 593: weight=1.0000000000151987\n",
      "iteration 594: weight=1.000000000013189\n",
      "iteration 595: weight=1.0000000000113185\n",
      "iteration 596: weight=1.000000000009582\n",
      "iteration 597: weight=1.0000000000079738\n",
      "iteration 598: weight=1.0000000000064888\n",
      "iteration 599: weight=1.0000000000051212\n",
      "iteration 600: weight=1.0000000000038658\n",
      "iteration 601: weight=1.0000000000027172\n",
      "iteration 602: weight=1.0000000000016698\n",
      "iteration 603: weight=1.0000000000007185\n",
      "iteration 604: weight=0.9999999999998581\n",
      "iteration 605: weight=0.9999999999990835\n",
      "iteration 606: weight=0.9999999999983898\n",
      "iteration 607: weight=0.9999999999977722\n",
      "iteration 608: weight=0.999999999997226\n",
      "iteration 609: weight=0.9999999999967466\n",
      "iteration 610: weight=0.9999999999963296\n",
      "iteration 611: weight=0.9999999999959708\n",
      "iteration 612: weight=0.999999999995666\n",
      "iteration 613: weight=0.9999999999954114\n",
      "iteration 614: weight=0.9999999999952033\n",
      "iteration 615: weight=0.999999999995038\n",
      "iteration 616: weight=0.999999999994912\n",
      "iteration 617: weight=0.999999999994822\n",
      "iteration 618: weight=0.999999999994765\n",
      "iteration 619: weight=0.9999999999947379\n",
      "iteration 620: weight=0.999999999994738\n",
      "iteration 621: weight=0.9999999999947625\n",
      "iteration 622: weight=0.999999999994809\n",
      "iteration 623: weight=0.9999999999948752\n",
      "iteration 624: weight=0.9999999999949587\n",
      "iteration 625: weight=0.9999999999950575\n",
      "iteration 626: weight=0.9999999999951696\n",
      "iteration 627: weight=0.9999999999952933\n",
      "iteration 628: weight=0.9999999999954268\n",
      "iteration 629: weight=0.9999999999955685\n",
      "iteration 630: weight=0.9999999999957171\n",
      "iteration 631: weight=0.9999999999958711\n",
      "iteration 632: weight=0.9999999999960292\n",
      "iteration 633: weight=0.9999999999961904\n",
      "iteration 634: weight=0.9999999999963536\n",
      "iteration 635: weight=0.9999999999965179\n",
      "iteration 636: weight=0.9999999999966823\n",
      "iteration 637: weight=0.9999999999968462\n",
      "iteration 638: weight=0.9999999999970088\n",
      "iteration 639: weight=0.9999999999971695\n",
      "iteration 640: weight=0.9999999999973277\n",
      "iteration 641: weight=0.9999999999974829\n",
      "iteration 642: weight=0.9999999999976348\n",
      "iteration 643: weight=0.9999999999977829\n",
      "iteration 644: weight=0.9999999999979269\n",
      "iteration 645: weight=0.9999999999980665\n",
      "iteration 646: weight=0.9999999999982015\n",
      "iteration 647: weight=0.9999999999983318\n",
      "iteration 648: weight=0.9999999999984571\n",
      "iteration 649: weight=0.9999999999985775\n",
      "iteration 650: weight=0.9999999999986927\n",
      "iteration 651: weight=0.9999999999988028\n",
      "iteration 652: weight=0.9999999999989078\n",
      "iteration 653: weight=0.9999999999990076\n",
      "iteration 654: weight=0.9999999999991023\n",
      "iteration 655: weight=0.999999999999192\n",
      "iteration 656: weight=0.9999999999992767\n",
      "iteration 657: weight=0.9999999999993565\n",
      "iteration 658: weight=0.9999999999994316\n",
      "iteration 659: weight=0.9999999999995018\n",
      "iteration 660: weight=0.9999999999995676\n",
      "iteration 661: weight=0.999999999999629\n",
      "iteration 662: weight=0.999999999999686\n",
      "iteration 663: weight=0.999999999999739\n",
      "iteration 664: weight=0.999999999999788\n",
      "iteration 665: weight=0.9999999999998331\n",
      "iteration 666: weight=0.9999999999998747\n",
      "iteration 667: weight=0.9999999999999126\n",
      "iteration 668: weight=0.9999999999999473\n",
      "iteration 669: weight=0.9999999999999788\n",
      "iteration 670: weight=1.0000000000000073\n",
      "iteration 671: weight=1.0000000000000329\n",
      "iteration 672: weight=1.0000000000000557\n",
      "iteration 673: weight=1.0000000000000762\n",
      "iteration 674: weight=1.0000000000000941\n",
      "iteration 675: weight=1.00000000000011\n",
      "iteration 676: weight=1.0000000000001235\n",
      "iteration 677: weight=1.0000000000001352\n",
      "iteration 678: weight=1.000000000000145\n",
      "iteration 679: weight=1.0000000000001532\n",
      "iteration 680: weight=1.0000000000001599\n",
      "iteration 681: weight=1.000000000000165\n",
      "iteration 682: weight=1.0000000000001688\n",
      "iteration 683: weight=1.0000000000001714\n",
      "iteration 684: weight=1.000000000000173\n",
      "iteration 685: weight=1.0000000000001734\n",
      "iteration 686: weight=1.000000000000173\n",
      "iteration 687: weight=1.0000000000001716\n",
      "iteration 688: weight=1.0000000000001696\n",
      "iteration 689: weight=1.000000000000167\n",
      "iteration 690: weight=1.0000000000001639\n",
      "iteration 691: weight=1.00000000000016\n",
      "iteration 692: weight=1.0000000000001559\n",
      "iteration 693: weight=1.0000000000001514\n",
      "iteration 694: weight=1.0000000000001465\n",
      "iteration 695: weight=1.0000000000001414\n",
      "iteration 696: weight=1.0000000000001361\n",
      "iteration 697: weight=1.0000000000001306\n",
      "iteration 698: weight=1.000000000000125\n",
      "iteration 699: weight=1.0000000000001195\n",
      "iteration 700: weight=1.0000000000001137\n",
      "iteration 701: weight=1.000000000000108\n",
      "iteration 702: weight=1.0000000000001021\n",
      "iteration 703: weight=1.0000000000000966\n",
      "iteration 704: weight=1.000000000000091\n",
      "iteration 705: weight=1.0000000000000855\n",
      "iteration 706: weight=1.0000000000000802\n",
      "iteration 707: weight=1.0000000000000748\n",
      "iteration 708: weight=1.0000000000000697\n",
      "iteration 709: weight=1.0000000000000648\n",
      "iteration 710: weight=1.00000000000006\n",
      "iteration 711: weight=1.0000000000000553\n",
      "iteration 712: weight=1.0000000000000508\n",
      "iteration 713: weight=1.0000000000000466\n",
      "iteration 714: weight=1.0000000000000426\n",
      "iteration 715: weight=1.0000000000000386\n",
      "iteration 716: weight=1.0000000000000349\n",
      "iteration 717: weight=1.0000000000000313\n",
      "iteration 718: weight=1.000000000000028\n",
      "iteration 719: weight=1.0000000000000249\n",
      "iteration 720: weight=1.000000000000022\n",
      "iteration 721: weight=1.000000000000019\n",
      "iteration 722: weight=1.0000000000000164\n",
      "iteration 723: weight=1.000000000000014\n",
      "iteration 724: weight=1.0000000000000118\n",
      "iteration 725: weight=1.0000000000000098\n",
      "iteration 726: weight=1.0000000000000078\n",
      "iteration 727: weight=1.000000000000006\n",
      "iteration 728: weight=1.0000000000000044\n",
      "iteration 729: weight=1.0000000000000029\n",
      "iteration 730: weight=1.0000000000000016\n",
      "iteration 731: weight=1.0000000000000002\n",
      "iteration 732: weight=0.9999999999999991\n",
      "iteration 733: weight=0.9999999999999981\n",
      "iteration 734: weight=0.9999999999999972\n",
      "iteration 735: weight=0.9999999999999964\n",
      "iteration 736: weight=0.9999999999999958\n",
      "iteration 737: weight=0.9999999999999952\n",
      "iteration 738: weight=0.9999999999999947\n",
      "iteration 739: weight=0.9999999999999942\n",
      "iteration 740: weight=0.9999999999999939\n",
      "iteration 741: weight=0.9999999999999936\n",
      "iteration 742: weight=0.9999999999999933\n",
      "iteration 743: weight=0.9999999999999931\n",
      "iteration 744: weight=0.999999999999993\n",
      "iteration 745: weight=0.9999999999999929\n",
      "converged after 746 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PCr\\AppData\\Local\\Temp\\ipykernel_6608\\1097381739.py:32: RuntimeWarning: invalid value encountered in sqrt\n",
      "  b = b - self.eta*(m_db_corr/(np.sqrt(v_db_corr)+self.epsilon))\n"
     ]
    }
   ],
   "source": [
    "w_0 = 0\n",
    "b_0 = 0\n",
    "adam = AdamOptim()\n",
    "t = 1 \n",
    "converged = False\n",
    "\n",
    "while not converged:\n",
    "    dw = grad_function(w_0)\n",
    "    db = grad_function(b_0)\n",
    "    w_0_old = w_0\n",
    "    w_0, b_0 = adam.update(t,w=w_0, b=b_0, dw=dw, db=db)\n",
    "    if check_convergence(w_0, w_0_old):\n",
    "        print('converged after '+str(t)+' iterations')\n",
    "        break\n",
    "    else:\n",
    "        print('iteration '+str(t)+': weight='+str(w_0))\n",
    "        t+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TvrDQgcTfeDx",
    "outputId": "dd45c00a-d4da-4215-aba2-2fc89012449d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2256 - accuracy: 0.9336\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0948 - accuracy: 0.9718\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0672 - accuracy: 0.9790\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0522 - accuracy: 0.9836\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0420 - accuracy: 0.9866\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0331 - accuracy: 0.9890\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0267 - accuracy: 0.9914\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0234 - accuracy: 0.9924\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0210 - accuracy: 0.9931\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0191 - accuracy: 0.9934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18e6e4121c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compilation du modele\n",
    "model.compile(optimizer='adam',\n",
    "              loss= keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entrainement du modele\n",
    "model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnifmnaOhodj"
   },
   "source": [
    "## 5. Évaluation du réseau de neurone sur les données de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3LdmrOwKg8nK",
    "outputId": "ab21b696-7ffa-4ad0-cafa-ac258a770f7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1061 - accuracy: 0.9754\n",
      "Test accuracy: 0.9753999710083008\n"
     ]
    }
   ],
   "source": [
    "# Evaluation du modele\n",
    "test_loss, test_acc = model.evaluate(X_test,  y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9a8vswFYhuGL"
   },
   "source": [
    "## 6. Création d'un modele prédictif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFv035qwg-Q6",
    "outputId": "38e6dd3a-b674-4e30-f87b-84247e779e37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n",
      "[7 2 1 0 4 1 4 9 5 9]\n",
      "[7 2 1 0 4 1 4 9 5 9]\n"
     ]
    }
   ],
   "source": [
    "# modele prédictif (softmax)\n",
    "prediction_model = keras.Sequential([model, keras.layers.Softmax()])\n",
    "predict_proba = prediction_model.predict(X_test)\n",
    "predictions = np.argmax(predict_proba, axis=1)\n",
    "\n",
    "print(predictions[:10])\n",
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOs8BB3zR8xdfuugq88TB1k",
   "include_colab_link": true,
   "name": "Tensorflow MNIST pour débutants.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
